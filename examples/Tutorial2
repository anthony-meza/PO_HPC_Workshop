# Tutorial 2: Xarray for Parallel Computing and Out-of-Memory Computations

# Import necessary libraries
import xarray as xr
import dask.array as da
import dask
from dask.distributed import Client
import matplotlib.pyplot as plt
import numpy as np
from scipy.signal import butter, filtfilt

# Initialize Dask client
client = Client()

# 1. Parallel computing
# Explanation of parallel computing concepts
print("Parallel computing allows computations to be performed simultaneously across multiple cores or processors.")

# 2. What are cores, nodes, CPUs
# Definitions and explanations
print("""
Cores: The basic computation units within a CPU.
Nodes: Physical or virtual machines in a computing cluster.
CPUs: Central Processing Units, which contain multiple cores.
""")

# 3. How does parallel computing work? Include a graph that illustrates how workers can access slices of data
# Illustration with a simple example
data = da.random.random((10000, 10000), chunks=(1000, 1000))
data_sum_serial = data.sum().compute()
data_sum_parallel = data.sum().compute()

# Plotting the graph (conceptual)
fig, ax = plt.subplots(1, 2, figsize=(12, 5))
ax[0].bar(['Serial'], [data_sum_serial])
ax[0].set_title('Sum in Serial')
ax[1].bar(['Parallel'], [data_sum_parallel])
ax[1].set_title('Sum in Parallel')
plt.show()

# 4. What is your maximum memory used given a dataset size, number of cores (i.e., workers), and chunk size
# Example calculation
dataset_size = 10000 * 10000  # size in elements
number_of_cores = 4
chunk_size = (1000, 1000)
memory_used = dataset_size * 8 / (1024**3) / number_of_cores  # assuming 8 bytes per element, converted to GB

print(f"Maximum memory used: {memory_used:.2f} GB")

# 5. Interactive: given dataset size and number of cores used, how big should your chunks be to stay within a memory threshold
# Function to calculate chunk size
def calculate_chunk_size(dataset_size, num_cores, memory_threshold):
    element_size = 8  # assuming 8 bytes per element
    max_elements_per_core = memory_threshold * (1024**3) / element_size
    total_elements = dataset_size / num_cores
    chunk_size = total_elements / max_elements_per_core
    return chunk_size

# Example usage
dataset_size = 10000 * 10000
num_cores = 4
memory_threshold = 2  # in GB

chunk_size = calculate_chunk_size(dataset_size, num_cores, memory_threshold)
print(f"Recommended chunk size: {chunk_size:.2f} elements per chunk")

# 6. Example: accessing a dataset
# Load dataset with Dask
ds = xr.tutorial.open_dataset('air_temperature').chunk({'time': 100})

# Perform a computation
mean_temp = ds.air.mean(dim='time').compute()
mean_temp.plot()
plt.title("Mean Air Temperature")
plt.show()

# 7. Optimizing “chunks”: How to access data optimally for your computational needs
# Explanation and example
print("Optimizing chunk size can improve performance by balancing memory usage and computation speed.")

# Example with different chunk sizes
ds_small_chunks = ds.chunk({'time': 10})
ds_large_chunks = ds.chunk({'time': 1000})

# Timing the computations
%timeit ds_small_chunks.air.mean(dim='time').compute()
%timeit ds_large_chunks.air.mean(dim='time').compute()

# 8. Accessing data along the wrong dimension can slow down your parallel computations
# Example to illustrate the impact
data_wrong_dim = da.random.random((10000, 10000), chunks=(1000, 10000))
%timeit data_wrong_dim.sum(axis=0).compute()

data_correct_dim = da.random.random((10000, 10000), chunks=(1000, 1000))
%timeit data_correct_dim.sum(axis=0).compute()

# Plotting to illustrate the impact
fig, ax = plt.subplots(1, 2, figsize=(12, 5))
ax[0].bar(['Wrong Dimension'], [data_wrong_dim.sum().compute()])
ax[0].set_title('Sum with Wrong Dimension')
ax[1].bar(['Correct Dimension'], [data_correct_dim.sum().compute()])
ax[1].set_title('Sum with Correct Dimension')
plt.show()

# 9. Using apply_ufunc to compute trends, correlations, and low-pass filters on a (lat, lon, time) dataset

# Load a sample dataset
ds = xr.tutorial.open_dataset('air_temperature').chunk({'time': 100})

# Define a function to compute the trend
def compute_trend(arr):
    time = np.arange(arr.shape[0])
    slope, intercept = np.polyfit(time, arr, 1)
    return slope

# Apply the function to compute the trend along the time dimension
trends = xr.apply_ufunc(
    compute_trend, ds.air,
    input_core_dims=[['time']],
    vectorize=True,
    dask='parallelized',
    output_dtypes=[float]
)

trends.plot()
plt.title("Trends in Air Temperature")
plt.show()

# Define a function to compute the correlation
def compute_correlation(arr1, arr2):
    return np.corrcoef(arr1, arr2)[0, 1]

# Generate a second dataset for correlation computation
ds2 = ds + xr.tutorial.open_dataset('air_temperature') * 0.1

# Apply the function to compute the correlation along the time dimension
correlations = xr.apply_ufunc(
    compute_correlation, ds.air, ds2.air,
    input_core_dims=[['time'], ['time']],
    vectorize=True,
    dask='parallelized',
    output_dtypes=[float]
)

correlations.plot()
plt.title("Correlations between Two Datasets")
plt.show()

# Define a function for low-pass filtering
def low_pass_filter(arr, cutoff):
    nyquist = 0.5 * len(arr)
    normal_cutoff = cutoff / nyquist
    b, a = butter(1, normal_cutoff, btype='low', analog=False)
    return filtfilt(b, a, arr)

# Apply the function to filter the data along the time dimension
filtered_data = xr.apply_ufunc(
    low_pass_filter, ds.air, 0.1,
    input_core_dims=[['time'], []],
    vectorize=True,
    dask='parallelized',
    output_dtypes=[float]
)

filtered_data.isel(time=0).plot()
plt.title("Low-Pass Filtered Air Temperature")
plt.show()
